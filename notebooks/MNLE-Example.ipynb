{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLE inference with varying numbers of trials\n",
    "\n",
    "This notebook presents are short summary of the code needed to train MNLE on the DDM and then perform inference with MCMC. \n",
    "\n",
    "The MNLE code itself and training and mcmc utils can be found in `mnle_utils.py` in this folder. \n",
    "\n",
    "If you have any questions please create an issue in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sbibm\n",
    "import sbi\n",
    "\n",
    "from mnle_utils import BernoulliMN, MNLE, train_choice_net\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('plotting_settings.mplstyle')\n",
    "# Colorblind color palette\n",
    "colors = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DDM simulator and prior from sbibm framework.\n",
    "task = sbibm.get_task(\"ddm\")\n",
    "prior = task.get_prior_dist()\n",
    "simulator = task.get_simulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ddm_training_data.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6fac1c51093b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ddm_training_data.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ddm_training_data.p'"
     ]
    }
   ],
   "source": [
    "# Generate training data (local Julia installation required) or load from disk.\n",
    "julia_available = False\n",
    "\n",
    "if julia_available:\n",
    "    N = 100000\n",
    "    theta = prior.sample((N,))\n",
    "    x = simulator(theta)\n",
    "else:\n",
    "    with open(\"ddm_training_data.p\", \"rb\") as fh:\n",
    "        theta, x, *_ = pickle.load(fh).values()\n",
    "        \n",
    "# The DDM simulator returns choices encoded as sign of the reaction times, decode:\n",
    "rts = abs(x)\n",
    "choices = torch.ones_like(x)\n",
    "choices[x < 0] = 0\n",
    "# Concatenate theta and choices for conditional flow training below.\n",
    "theta_and_choices = torch.cat((theta, choices), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 3\n",
    "num_hidden_units = 10\n",
    "validation_fraction = 0.1\n",
    "stop_after_epochs = 20\n",
    "training_batch_size = 100\n",
    "\n",
    "# for neural spline flow\n",
    "use_log_rts = True\n",
    "num_transforms = 2\n",
    "num_bins = 5\n",
    "base_distribution = \"gaussian\"\n",
    "tails = \"linear\"\n",
    "tail_bound = 10\n",
    "tail_bound_eps = 1e-7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train separate likelihood estimators for choices and reaction times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train choice net.\n",
    "choice_net, vallp = train_choice_net(\n",
    "    theta,\n",
    "    choices,\n",
    "    # set up NN to learn Bernoulli probs over choices.\n",
    "    net=BernoulliMN(\n",
    "        n_hidden_layers=num_hidden_layers, n_hidden_units=num_hidden_units\n",
    "    ),\n",
    "    validation_fraction=validation_fraction,\n",
    "    stop_after_epochs=stop_after_epochs,\n",
    "    batch_size=training_batch_size,\n",
    ")\n",
    "\n",
    "## train flow using sbi routines.\n",
    "# construct the density estimator.\n",
    "density_estimator_fun = likelihood_nn(\n",
    "    model=\"nsf\",\n",
    "    num_transforms=num_transforms,\n",
    "    hidden_features=num_hidden_units,\n",
    "    num_bins=num_bins,\n",
    "    base_distribution=base_distribution,\n",
    "    tails=tails,\n",
    "    tail_bound=tail_bound,\n",
    "    tail_bound_eps=tail_bound_eps,\n",
    "    num_hidden_spline_context_layers=num_hidden_layers,\n",
    ")\n",
    "\n",
    "# set up sbi training object.\n",
    "inference_method = inference.SNLE(\n",
    "    density_estimator=density_estimator_fun,\n",
    "    prior=prior,\n",
    ")\n",
    "# append data and train\n",
    "inference_method = inference_method.append_simulations(\n",
    "    theta=theta_and_choices,\n",
    "    x=torch.log(rts) if use_log_rts else rts,\n",
    "    from_round=0,\n",
    ")\n",
    "rt_flow = inference_method.train(\n",
    "    training_batch_size=training_batch_size,\n",
    "    show_train_summary=False,\n",
    "    stop_after_epochs=stop_after_epochs,\n",
    ")\n",
    "\n",
    "mnle = MNLE(choice_net, rt_flow, use_log_rts=use_log_rts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned likelihood estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lower_bound = 1e-7\n",
    "test_theta = prior.sample((1,))\n",
    "test_data = np.arange(-5, 5, 1000)\n",
    "# Separate rts and choices.\n",
    "rts = abs(test_data)\n",
    "cs = torch.ones_like(test_data)\n",
    "cs[x < 0] = 0\n",
    "\n",
    "\n",
    "analytical_likelihoods = torch.tensor([task.get_log_likelihood(test_theta, \n",
    "                                                               test_rt.reshape(-1, 1), \n",
    "                                                               l_lower_bound=l_lower_bound) \n",
    "                                       for test_rt in test_rts])\n",
    "\n",
    "mnle_likelihoods = torch.tensor([mnle.log_prob(r.reshape(-1, 1), \n",
    "                                               c.reshape(-1, 1), \n",
    "                                               theta_o)\n",
    "                                 for r, c in zip(rs, cs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=15, 5)\n",
    "plt.plot(test_data, analytical_likelihoods.exp(), label=\"Analytical L\", c=colors[0]);\n",
    "plt.plot(test_data, mnle_likelihoods.exp(), label=\"MNLE\", ls=\"-\", c=colors[2]);\n",
    "plt.ylabel(r\"$L(x | \\theta)$\");\n",
    "plt.xlabel(\"reaction time [s]\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observations with different numbers of trials from DDM benchmark\n",
    "xo1 = task.get_observation(1)\n",
    "xo10 = task.get_observation(101)\n",
    "xo100 = task.get_observation(201)\n",
    "num_samples = 1000\n",
    "\n",
    "potential_fun = mnle.get_potential_fn(data=xo1, prior=prior, transforms=task.get_transforms())\n",
    "mcmc_parameters = dict(num_chains=10, warmup_steps=100, thin=10, init_strategy=\"prior\")\n",
    "posterior_samples = run_mcmc(prior, potential_fun, mcmc_parameters, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 1  # change to 101 or 201 depending on observation chosen above.\n",
    "reference_posterior_samples = task.get_reference_posterior_samples(obs)\n",
    "\n",
    "fig, ax1 = pairplot([reference_posterior_samples, posterior_samples],\n",
    "         points=sbibm.get_task(\"ddm\").get_true_parameters(obs), \n",
    "         limits=[[-2, 2], [0.5, 2.0], [.3, .7], [.2, 1.8]], \n",
    "         ticks = [[-2, 2], [0.5, 2.0], [.3, .7], [.2, 1.8]], \n",
    "         samples_colors=colors[:2], \n",
    "         diag=\"kde\",\n",
    "         upper=\"contour\",\n",
    "         kde_offdiag=dict(bw_method=\"scott\", bins=50),\n",
    "         contour_upper=dict(levels=[0.1], percentile=False),\n",
    "         points_offdiag=dict(marker=\"+\", markersize=10), \n",
    "         points_colors=[\"k\"], \n",
    "         labels=[r\"$v$\", r\"$a$\", r\"$w$\", r\"$\\tau$\"])\n",
    "\n",
    "plt.sca(ax1[0, 0])\n",
    "plt.legend([\"Reference\", \"MNLE\", r\"Ground truth $\\theta$\"], \n",
    "           bbox_to_anchor=(-.1, -2.2), \n",
    "           loc=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
